# ğŸ›ï¸ Otto Group Product Classification

## ğŸ“Œ Overview
This repository contains a classification model built to predict **product categories** for the **Otto Group Product Dataset**. We leverage various **machine learning algorithms** and a **deep learning model** to achieve high accuracy in classifying products into **nine distinct classes**.

ğŸ“ **Original Notebook:** [Kaggle Notebook](https://www.kaggle.com/code/senasudemir/otto-group-product-classification#Conclusion)

---

## ğŸ“Š Dataset Description
The dataset consists of numerical features representing various product attributes and a categorical **target variable**.

- **ğŸ”¢ Features:** `feat_1` to `feat_93` (Numerical features)
- **ğŸ·ï¸ Target Variable:** `target` (One of `Class_1` to `Class_9`)
- **ğŸ†” Identifier:** `id` (Unique ID for each observation)

---

## ğŸ¯ Goal
The objective is to train classification models that **accurately predict** the product class based on the given feature set.

---

## ğŸš€ Models & Accuracy Scores

| Model | Accuracy Score |
|--------|--------------|
| **KNeighbors Classifier** | ğŸ¯ **0.999677** |
| **Gradient Boosting Classifier** | ğŸ¯ **0.999677** |
| **Decision Tree Classifier** | ğŸ¯ **0.999677** |
| **Random Forest Classifier** | ğŸŒ² **0.993293** |
| **Gaussian NB** | ğŸ”µ **0.813752** |
| **Logistic Regression** | â• **0.777796** |
| **Bernoulli NB** | ğŸ”´ **0.614415** |
| **Deep Learning Model** | ğŸ§  **0.979072** |

---

## ğŸ”‘ Key Takeaways
âœ… **Top Performers:**  
- **KNeighbors, Gradient Boosting, and Decision Tree classifiers** reached an exceptional **99.96%** accuracy.  
- **Random Forest** followed closely with **99.33%** accuracy.  

âš ï¸ **Weaker Models:**  
- **NaÃ¯ve Bayes models (Gaussian NB & Bernoulli NB)** had significantly lower performance, indicating the dataset may not align well with probabilistic approaches.  
- **Logistic Regression** achieved a modest **77.78%**, suggesting linear models are not the best fit.  

ğŸ§  **Deep Learning Model:**  
- The **neural network** performed well with **97.90%** accuracy but was slightly outperformed by traditional models.  
- Further **hyperparameter tuning** and **advanced architectures** could improve its performance.
- 
---
